{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "5af03e6f6884cdcbf495aa98cbd8d63d5d0ed05506dbd026ba0a4981213ae30b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parsing page 1 of pdf file...\n",
      "Parsing page 2 of pdf file...\n",
      "Parsing page 3 of pdf file...\n",
      "Parsing page 4 of pdf file...\n",
      "Parsing page 5 of pdf file...\n",
      "Parsing page 6 of pdf file...\n",
      "Parsing page 7 of pdf file...\n",
      "Parsing page 8 of pdf file...\n",
      "Parsing page 9 of pdf file...\n",
      "Parsing page 10 of pdf file...\n",
      "Parsing page 11 of pdf file...\n",
      "Parsing page 12 of pdf file...\n",
      "Parsing page 13 of pdf file...\n",
      "Parsing page 14 of pdf file...\n",
      "Parsing page 15 of pdf file...\n",
      "Parsing page 16 of pdf file...\n",
      "Parsing page 17 of pdf file...\n",
      "Parsing page 18 of pdf file...\n",
      "Parsing page 19 of pdf file...\n",
      "Parsing page 20 of pdf file...\n",
      "Parsing page 21 of pdf file...\n",
      "Parsing page 22 of pdf file...\n",
      "Parsing page 23 of pdf file...\n",
      "Parsing page 24 of pdf file...\n",
      "Parsing page 25 of pdf file...\n",
      "Parsing page 26 of pdf file...\n",
      "Parsing page 27 of pdf file...\n",
      "Parsing page 28 of pdf file...\n",
      "Parsing page 29 of pdf file...\n",
      "Parsing page 30 of pdf file...\n",
      "Parsing page 31 of pdf file...\n",
      "Parsing page 32 of pdf file...\n",
      "Parsing page 33 of pdf file...\n",
      "Parsing page 34 of pdf file...\n",
      "Parsing page 35 of pdf file...\n",
      "Parsing page 36 of pdf file...\n",
      "Parsing page 37 of pdf file...\n",
      "Parsing page 38 of pdf file...\n",
      "Parsing page 39 of pdf file...\n",
      "Parsing page 40 of pdf file...\n",
      "Parsing page 41 of pdf file...\n",
      "Parsing page 42 of pdf file...\n",
      "Parsing page 43 of pdf file...\n",
      "Parsing page 44 of pdf file...\n",
      "Parsing page 45 of pdf file...\n",
      "Parsing page 46 of pdf file...\n",
      "Parsing page 47 of pdf file...\n",
      "Parsing page 48 of pdf file...\n",
      "Parsing page 49 of pdf file...\n",
      "Parsing page 50 of pdf file...\n",
      "Parsing page 51 of pdf file...\n",
      "Parsing page 52 of pdf file...\n",
      "Parsing page 53 of pdf file...\n",
      "Parsing page 54 of pdf file...\n",
      "Parsing page 55 of pdf file...\n",
      "Parsing page 56 of pdf file...\n",
      "Parsing page 57 of pdf file...\n",
      "Parsing page 58 of pdf file...\n",
      "Parsing page 59 of pdf file...\n",
      "Parsing page 60 of pdf file...\n",
      "Parsing page 61 of pdf file...\n",
      "Parsing page 62 of pdf file...\n",
      "Parsing page 63 of pdf file...\n",
      "Parsing page 64 of pdf file...\n",
      "Parsing page 65 of pdf file...\n",
      "Parsing page 66 of pdf file...\n",
      "Parsing page 67 of pdf file...\n",
      "Parsing page 68 of pdf file...\n",
      "Parsing page 69 of pdf file...\n",
      "Parsing page 70 of pdf file...\n",
      "Parsing page 71 of pdf file...\n",
      "Parsing page 72 of pdf file...\n",
      "Parsing page 73 of pdf file...\n",
      "Parsing page 74 of pdf file...\n",
      "Parsing page 75 of pdf file...\n",
      "Parsing page 76 of pdf file...\n",
      "Parsing page 77 of pdf file...\n",
      "Parsing page 78 of pdf file...\n",
      "Parsing page 79 of pdf file...\n",
      "Parsing page 80 of pdf file...\n",
      "Parsing page 81 of pdf file...\n",
      "Parsing page 82 of pdf file...\n",
      "Parsing page 83 of pdf file...\n",
      "Parsing page 84 of pdf file...\n",
      "Parsing page 85 of pdf file...\n",
      "Parsing page 86 of pdf file...\n",
      "Parsing page 87 of pdf file...\n",
      "Parsing page 88 of pdf file...\n",
      "Parsing page 89 of pdf file...\n",
      "Parsing page 90 of pdf file...\n",
      "Parsing page 91 of pdf file...\n",
      "Parsing page 92 of pdf file...\n",
      "Parsing page 93 of pdf file...\n",
      "Parsing page 94 of pdf file...\n",
      "Parsing page 95 of pdf file...\n",
      "Parsing page 96 of pdf file...\n",
      "Parsing page 97 of pdf file...\n",
      "Parsing page 98 of pdf file...\n",
      "Parsing page 99 of pdf file...\n",
      "Parsing page 100 of pdf file...\n",
      "Parsing page 101 of pdf file...\n",
      "Parsing page 102 of pdf file...\n",
      "Parsing page 103 of pdf file...\n",
      "Parsing page 104 of pdf file...\n",
      "Parsing page 105 of pdf file...\n",
      "Parsing page 106 of pdf file...\n",
      "Parsing page 107 of pdf file...\n",
      "Parsing page 108 of pdf file...\n",
      "Parsing page 109 of pdf file...\n",
      "Parsing page 110 of pdf file...\n",
      "Parsing page 111 of pdf file...\n",
      "Parsing page 112 of pdf file...\n",
      "Parsing page 113 of pdf file...\n",
      "Parsing page 114 of pdf file...\n",
      "Parsing page 115 of pdf file...\n",
      "Parsing page 116 of pdf file...\n",
      "Parsing page 117 of pdf file...\n",
      "Parsing page 118 of pdf file...\n",
      "Parsing page 119 of pdf file...\n",
      "Parsing page 120 of pdf file...\n",
      "Parsing page 121 of pdf file...\n",
      "Parsing page 122 of pdf file...\n",
      "Parsing page 123 of pdf file...\n",
      "Parsing page 124 of pdf file...\n",
      "Parsing page 125 of pdf file...\n",
      "Parsing page 126 of pdf file...\n",
      "Parsing page 127 of pdf file...\n",
      "Parsing page 128 of pdf file...\n",
      "Parsing page 129 of pdf file...\n",
      "Parsing page 130 of pdf file...\n",
      "Parsing page 131 of pdf file...\n",
      "Parsing page 132 of pdf file...\n",
      "Parsing page 133 of pdf file...\n",
      "Parsing page 134 of pdf file...\n",
      "Parsing page 135 of pdf file...\n",
      "Parsing page 136 of pdf file...\n",
      "Parsing page 137 of pdf file...\n",
      "Parsing page 138 of pdf file...\n",
      "Parsing page 139 of pdf file...\n",
      "Parsing page 140 of pdf file...\n",
      "Parsing page 141 of pdf file...\n",
      "Parsing page 142 of pdf file...\n",
      "Parsing page 143 of pdf file...\n",
      "Parsing page 144 of pdf file...\n",
      "Parsing page 145 of pdf file...\n",
      "Parsing page 146 of pdf file...\n",
      "Parsing page 147 of pdf file...\n",
      "Parsing page 148 of pdf file...\n",
      "Parsing page 149 of pdf file...\n",
      "Parsing page 150 of pdf file...\n",
      "Parsing page 151 of pdf file...\n",
      "Parsing page 152 of pdf file...\n",
      "Parsing page 153 of pdf file...\n",
      "Parsing page 154 of pdf file...\n",
      "Parsing page 155 of pdf file...\n",
      "Parsing page 156 of pdf file...\n",
      "Parsing page 157 of pdf file...\n",
      "Parsing page 158 of pdf file...\n",
      "Parsing page 159 of pdf file...\n",
      "Parsing page 160 of pdf file...\n",
      "Parsing page 161 of pdf file...\n",
      "Parsing page 162 of pdf file...\n",
      "Parsing page 163 of pdf file...\n",
      "Parsing page 164 of pdf file...\n",
      "Parsing page 165 of pdf file...\n",
      "Parsing page 166 of pdf file...\n",
      "Parsing page 167 of pdf file...\n",
      "Parsing page 168 of pdf file...\n",
      "Parsing page 169 of pdf file...\n",
      "Parsing page 170 of pdf file...\n",
      "Parsing page 171 of pdf file...\n",
      "Parsing page 172 of pdf file...\n",
      "Parsing page 173 of pdf file...\n",
      "Parsing page 174 of pdf file...\n",
      "Parsing page 175 of pdf file...\n",
      "Parsing page 176 of pdf file...\n",
      "Parsing page 177 of pdf file...\n",
      "Parsing page 178 of pdf file...\n",
      "Parsing page 179 of pdf file...\n",
      "Parsing page 180 of pdf file...\n",
      "Parsing page 181 of pdf file...\n",
      "Parsing page 182 of pdf file...\n",
      "Parsing page 183 of pdf file...\n",
      "Parsing page 184 of pdf file...\n",
      "Parsing page 185 of pdf file...\n",
      "Parsing page 186 of pdf file...\n",
      "Parsing page 187 of pdf file...\n",
      "Parsing page 188 of pdf file...\n",
      "Parsing page 189 of pdf file...\n",
      "Parsing page 190 of pdf file...\n",
      "Parsing page 191 of pdf file...\n",
      "Parsing page 192 of pdf file...\n",
      "Parsing page 193 of pdf file...\n",
      "Parsing page 194 of pdf file...\n",
      "Parsing page 195 of pdf file...\n",
      "Parsing page 196 of pdf file...\n",
      "Parsing page 197 of pdf file...\n",
      "Parsing page 198 of pdf file...\n",
      "Parsing page 199 of pdf file...\n",
      "Parsing page 200 of pdf file...\n",
      "Parsing page 201 of pdf file...\n",
      "Parsing page 202 of pdf file...\n",
      "Parsing page 203 of pdf file...\n",
      "Parsing page 204 of pdf file...\n",
      "Parsing page 205 of pdf file...\n",
      "Parsing page 206 of pdf file...\n",
      "Parsing page 207 of pdf file...\n",
      "Parsing page 208 of pdf file...\n",
      "Parsing page 209 of pdf file...\n",
      "Parsing page 210 of pdf file...\n",
      "Parsing page 211 of pdf file...\n",
      "Parsing page 212 of pdf file...\n",
      "Parsing page 213 of pdf file...\n",
      "Parsing page 214 of pdf file...\n",
      "Parsing page 215 of pdf file...\n",
      "Parsing page 216 of pdf file...\n",
      "Parsing page 217 of pdf file...\n",
      "Parsing page 218 of pdf file...\n",
      "Parsing page 219 of pdf file...\n",
      "Parsing page 220 of pdf file...\n",
      "Parsing page 221 of pdf file...\n",
      "Parsing page 222 of pdf file...\n",
      "Parsing page 223 of pdf file...\n",
      "Parsing page 224 of pdf file...\n",
      "Parsing page 225 of pdf file...\n",
      "Parsing page 226 of pdf file...\n",
      "Parsing page 227 of pdf file...\n",
      "Parsing page 228 of pdf file...\n",
      "Parsing page 229 of pdf file...\n",
      "Parsing page 230 of pdf file...\n",
      "Parsing page 231 of pdf file...\n",
      "Parsing page 232 of pdf file...\n",
      "Parsing page 233 of pdf file...\n",
      "Parsing page 234 of pdf file...\n",
      "Parsing page 235 of pdf file...\n",
      "Parsing page 236 of pdf file...\n",
      "Parsing page 237 of pdf file...\n",
      "Parsing page 238 of pdf file...\n",
      "Parsing page 239 of pdf file...\n",
      "Parsing page 240 of pdf file...\n",
      "Parsing page 241 of pdf file...\n",
      "Parsing page 242 of pdf file...\n",
      "Parsing page 243 of pdf file...\n",
      "Parsing page 244 of pdf file...\n",
      "Parsing page 245 of pdf file...\n",
      "Parsing page 246 of pdf file...\n",
      "Parsing page 247 of pdf file...\n",
      "Parsing page 248 of pdf file...\n",
      "Parsing page 249 of pdf file...\n",
      "Parsing page 250 of pdf file...\n",
      "Parsing page 251 of pdf file...\n",
      "Parsing page 252 of pdf file...\n",
      "Parsing page 253 of pdf file...\n",
      "Parsing page 254 of pdf file...\n",
      "Parsing page 255 of pdf file...\n",
      "Parsing page 256 of pdf file...\n",
      "Parsing page 257 of pdf file...\n",
      "Parsing page 258 of pdf file...\n",
      "Parsing page 259 of pdf file...\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from tika import parser\n",
    "import re\n",
    "\n",
    "file_path = \"../Books Index for Phrases/JAMES R HEDGESHedges On Hedge Funds How To Successfully Analyze AND SELECT AN INVESTMENT.pdf\"\n",
    "\n",
    "file_data = []\n",
    "_buffer = StringIO()\n",
    "data = parser.from_file(file_path, xmlContent=True)\n",
    "xhtml_data = BeautifulSoup(data['content'])\n",
    "for page, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "    print('Parsing page {} of pdf file...'.format(page+1))\n",
    "    _buffer = StringIO()\n",
    "    _buffer.write(str(content))\n",
    "    parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "    _buffer.truncate()\n",
    "    file_data.append({'id': 'page_'+str(page+1), 'content': parsed_content['content']})"
   ]
  },
  {
   "source": [
    "## Glossary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Glossary ## \n",
    "## Page 199 (226) to Page 215 (242)\n",
    "glossary_pages = xhtml_data.find_all('div', attrs={'class': 'page'})[225: 242]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_page_1 = glossary_pages[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<div class=\"page\"><p></p>\n",
       "<p>Risk premium The extra rate of return required to attract investors to an\n",
       "asset due to the incremental risk incurred from investing in it. \n",
       "</p>\n",
       "<p>Rolling rate of return The average return of a rolling performance period.\n",
       "</p>\n",
       "<p>R-squared (coefficient of determination) A measure of how well a regres-\n",
       "sion line fits the data. It indicates the percent of variation in the data\n",
       "that is explained by the regression line. R-squared can vary between 0\n",
       "and 1, where 1 indicates that 100 percent of the variation in the invest-\n",
       "ment returns (dependent variable) is explained by the regression for the\n",
       "period measured. \n",
       "</p>\n",
       "<p>Russell 1000® Index An index consisting of the 1,000 largest companies\n",
       "in the Russell 3000 Index, representing 89 percent of the total market\n",
       "capitalization of the Russell 3000. \n",
       "</p>\n",
       "<p>Russell 2000® Growth Index An index containing those Russell 2000\n",
       "securities with a greater-than-average growth orientation. Securities in\n",
       "this index generally have higher price-to-book and price-to-earnings\n",
       "ratios than those in the Russell 2000 Value Index. \n",
       "</p>\n",
       "<p>Russell 2000® Small Stock Index An index comprised of the 2,000 small-\n",
       "est securities in the Russell 3000 Index, and includes reinvestment of\n",
       "dividends. It represents approximately 7 percent of the Russell 3000. \n",
       "</p>\n",
       "<p>Russell 2000® Value Index An index containing those Russell 2000 secu-\n",
       "rities with a less-than-average growth orientation. Securities in this\n",
       "index generally have lower price-to-book and price-to-earnings ratios\n",
       "than those in the Russell 2000 Growth Index. \n",
       "</p>\n",
       "<p>Russell 3000® Index An index that measures the performance of the 3,000\n",
       "largest U.S. companies based on total market capitalization, which rep-\n",
       "resents approximately 98 percent of the investable U.S. equity market. \n",
       "</p>\n",
       "<p>Sector funds An investment strategy that takes long and/or short positions\n",
       "in the companies of specific sectors of the economy, such as biotech-\n",
       "nology, financials, and information technology.\n",
       "</p>\n",
       "<p>Sharpe ratio A ratio calculated by subtracting the risk-free (Treasury bill)\n",
       "rate from a portfolio’s total return and then dividing this by its standard\n",
       "deviation. Because the numerator is the portfolio’s risk premium, the\n",
       "resulting fraction indicates the risk premium return earned per unit of\n",
       "total risk. It measures the reward-to-risk efficiency of an investment.\n",
       "</p>\n",
       "<p>212 GLOSSARY \n",
       "</p>\n",
       "<p>bgloss.qxd  8/30/04  12:15 PM  Page 212</p>\n",
       "<p></p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "glossary_page_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Risk premium\nThe extra rate of return required to attract investors to an asset due to the incremental risk incurred from investing in it.\nRolling rate of return\nThe average return of a rolling performance period.\nR-squared (coefficient of determination)\nA measure of how well a regres- sion line fits the data. It indicates the percent of variation in the data that is explained by the regression line. R-squared can vary between 0 and 1, where 1 indicates that 100 percent of the variation in the invest- ment returns (dependent variable) is explained by the regression for the period measured.\nRussell 1000\nIndex An index consisting of the 1,000 largest companies in the Russell 3000 Index, representing 89 percent of the total market capitalization of the Russell 3000.\nRussell 2000\nGrowth Index An index containing those Russell 2000 securities with a greater-than-average growth orientation. Securities in this index generally have higher price-to-book and price-to-earnings ratios than those in the Russell 2000 Value Index.\nRussell 2000\nSmall Stock Index An index comprised of the 2,000 small- est securities in the Russell 3000 Index, and includes reinvestment of dividends. It represents approximately 7 percent of the Russell 3000.\nRussell 2000\nValue Index An index containing those Russell 2000 secu- rities with a less-than-average growth orientation. Securities in this index generally have lower price-to-book and price-to-earnings ratios than those in the Russell 2000 Growth Index.\nRussell 3000\nIndex An index that measures the performance of the 3,000 largest U.S. companies based on total market capitalization, which rep- resents approximately 98 percent of the investable U.S. equity market.\nSector funds\nAn investment strategy that takes long and/or short positions in the companies of specific sectors of the economy, such as biotech- nology, financials, and information technology.\nSharpe ratio\nA ratio calculated by subtracting the risk-free (Treasury bill) rate from a portfolios total return and then dividing this by its standard deviation. Because the numerator is the portfolios risk premium, the resulting fraction indicates the risk premium return earned per unit of total risk. It measures the reward-to-risk efficiency of an investment.\n"
     ]
    }
   ],
   "source": [
    "for p in glossary_page_1.findAll(name='p'):\n",
    "    # print(p.text)\n",
    "    text = p.text\n",
    "    text = text.replace('’', '')\n",
    "    text = text.replace('®', '')\n",
    "    text = text.replace('- ', '-')\n",
    "\n",
    "    # remove Page number \"199\", \"200\" ... \n",
    "    if '.' not in text:\n",
    "        continue\n",
    "\n",
    "    ## remove page number\n",
    "    if re.search(r'Page \\d{3}', text): \n",
    "        continue\n",
    "\n",
    "    ## remove Glossary \n",
    "    if re.search(r'Glossary', text): \n",
    "        continue\n",
    "\n",
    "    ## remove Adapted from Appendix E\n",
    "    if re.search(r'Adapted from Appendix E', text): \n",
    "        continue\n",
    "\n",
    "    ## pass is the first letter is not capital letter\n",
    "    if not re.search(r'^[A-Z]', text):\n",
    "        continue\n",
    "    \n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    # search for the start of explanation\n",
    "    start = re.search(' [A-Z]', text).span()[0]\n",
    "    phrase = text[:start]\n",
    "    explanation = text[start+1:]\n",
    "    \n",
    "    if text != ['']:\n",
    "        print(phrase)\n",
    "        print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = []\n",
    "PHRASES = []\n",
    "EXPLANATION = []\n",
    "\n",
    "for page in glossary_pages:\n",
    "\n",
    "    for p in page.findAll(name='p'):\n",
    "        # print(p.text)\n",
    "        text = p.text\n",
    "        text = text.replace('’', \"'\")\n",
    "        text = text.replace('–', '-')\n",
    "        text = text.replace('®', '')\n",
    "        text = text.replace('- ', '-')\n",
    "\n",
    "        # remove Page number \"199\", \"200\" ... \n",
    "        if '.' not in text:\n",
    "            continue\n",
    "\n",
    "        ## remove page number\n",
    "        if re.search(r'Page \\d{3}', text): \n",
    "            continue\n",
    "\n",
    "        ## remove Glossary \n",
    "        if re.search(r'Glossary', text): \n",
    "            continue\n",
    "\n",
    "        ## remove Adapted from Appendix E\n",
    "        if re.search(r'Adapted from Appendix E', text): \n",
    "            continue\n",
    "\n",
    "        ## pass is the first letter is not capital letter\n",
    "        if not re.search(r'^[A-Z]', text):\n",
    "            continue\n",
    "        \n",
    "        text = text.replace('\\n', ' ').strip()\n",
    "        text_original = text\n",
    "\n",
    "        if text: \n",
    "            # search for the start of explanation\n",
    "            start = re.search(' [A-Z]', text).span()[0]\n",
    "            phrase = text[:start].strip()\n",
    "            explanation = text[start+1:].strip()\n",
    "            TEXT.append(text_original)\n",
    "            PHRASES.append(phrase)\n",
    "            EXPLANATION.append(explanation)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame({'ID': [i for i in range(len(PHRASES))],\n",
    "            'original_glossary_extracted': TEXT,\n",
    "            'key_phrase': PHRASES,\n",
    "            'source_book': ['JAMES R HEDGES Hedges On Hedge Funds How To Successfully Analyze AND SELECT AN INVESTMENT.pdf' for _ in range(len(PHRASES))],\n",
    "            'definition_or_explanation': EXPLANATION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   ID                        original_glossary_extracted  \\\n",
       "0   0  Accredited investor An individual (1) who has ...   \n",
       "1   1  Administrator A third-party service provider t...   \n",
       "2   2  Alternative investments The alternative invest...   \n",
       "3   3  ADV A form that all Registered Investment Advi...   \n",
       "4   4  Alpha A numerical value indicating excess rate...   \n",
       "\n",
       "                key_phrase                                        source_book  \\\n",
       "0      Accredited investor  JAMES R HEDGES Hedges On Hedge Funds How To Su...   \n",
       "1            Administrator  JAMES R HEDGES Hedges On Hedge Funds How To Su...   \n",
       "2  Alternative investments  JAMES R HEDGES Hedges On Hedge Funds How To Su...   \n",
       "3                      ADV  JAMES R HEDGES Hedges On Hedge Funds How To Su...   \n",
       "4                    Alpha  JAMES R HEDGES Hedges On Hedge Funds How To Su...   \n",
       "\n",
       "                           definition_or_explanation  \n",
       "0  An individual (1) who has made $200,000 per ye...  \n",
       "1  A third-party service provider that maintains ...  \n",
       "2  The alternative investment universe consists o...  \n",
       "3  A form that all Registered Investment Advisors...  \n",
       "4  A numerical value indicating excess rate of re...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>original_glossary_extracted</th>\n      <th>key_phrase</th>\n      <th>source_book</th>\n      <th>definition_or_explanation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Accredited investor An individual (1) who has ...</td>\n      <td>Accredited investor</td>\n      <td>JAMES R HEDGES Hedges On Hedge Funds How To Su...</td>\n      <td>An individual (1) who has made $200,000 per ye...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Administrator A third-party service provider t...</td>\n      <td>Administrator</td>\n      <td>JAMES R HEDGES Hedges On Hedge Funds How To Su...</td>\n      <td>A third-party service provider that maintains ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Alternative investments The alternative invest...</td>\n      <td>Alternative investments</td>\n      <td>JAMES R HEDGES Hedges On Hedge Funds How To Su...</td>\n      <td>The alternative investment universe consists o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>ADV A form that all Registered Investment Advi...</td>\n      <td>ADV</td>\n      <td>JAMES R HEDGES Hedges On Hedge Funds How To Su...</td>\n      <td>A form that all Registered Investment Advisors...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Alpha A numerical value indicating excess rate...</td>\n      <td>Alpha</td>\n      <td>JAMES R HEDGES Hedges On Hedge Funds How To Su...</td>\n      <td>A numerical value indicating excess rate of re...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "## Reformat extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_full_expression_and_abbreviation(text):\n",
    "    text_length = len(text)\n",
    "    match = re.search('\\(.+\\)', text) # search of bracket\n",
    "    match_1 = re.search('[A-Z]{2,8}', text) # search for Capitalization\n",
    "    match_2 = re.search('[A-Z]/[A-Z]', text) # search for Capitalization\n",
    "    if match and (match_1 or match_2):\n",
    "        content_in_bracket_length = len(match.group(0)) - 2\n",
    "    else:\n",
    "        if re.search('\\(.+', text):\n",
    "            pos = re.search('\\(.+', text).span()[0]\n",
    "            return [text[:pos-1]]\n",
    "\n",
    "        else:\n",
    "            return [text]\n",
    "\n",
    "    start, end = re.search('\\(.+\\)', text).span()\n",
    "    if content_in_bracket_length > text_length / 2:\n",
    "        \n",
    "        full_expression = text[start+1: end-1]\n",
    "        abbreviated_expression = text[: start-1]\n",
    "    else:\n",
    "        full_expression = text[: start-1]\n",
    "        abbreviated_expression = text[start+1: end-1]\n",
    "\n",
    "    return [full_expression, abbreviated_expression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = []\n",
    "extracted_acronym_1 = []\n",
    "extracted_acronym_2 = []\n",
    "\n",
    "for phrase, definition  in zip(df['key_phrase'].values, df['definition_or_explanation'].values):\n",
    "     res = find_full_expression_and_abbreviation(phrase)\n",
    "     if len(res) == 1:\n",
    "         full.append(res[0])\n",
    "         extracted_acronym_1.append('')\n",
    "         extracted_acronym_2.append('')\n",
    "\n",
    "     if len(res) == 2:\n",
    "         full.append(res[0])\n",
    "         extracted_acronym_1.append(res[1])\n",
    "         extracted_acronym_2.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['key_phrase'] = full\n",
    "df['extracted_acronym_1'] = extracted_acronym_1\n",
    "df['extracted_acronym_2'] = extracted_acronym_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_key_phrase(phrase):\n",
    "    phrase = phrase.replace('-', ' ')\n",
    "    phrase = phrase.replace('/', ' ')\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['key_phrase'] = df['key_phrase'].apply(clean_extracted_key_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['ID', 'original_glossary_extracted', 'key_phrase', 'source_book',\n",
       "       'definition_or_explanation', 'extracted_acronym_1',\n",
       "       'extracted_acronym_2'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['ID', 'original_glossary_extracted', 'key_phrase', 'extracted_acronym_1',\n",
    "       'extracted_acronym_2', 'definition_or_explanation', 'source_book',\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('glossary_extraction_JAMES R HEDGES Hedges On Hedge Funds How To Successfully Analyze AND SELECT AN INVESTMENT.pdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}